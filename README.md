# ğŸ“Š Gamboge ETL Pipeline

## Overview

This project implements an **end-to-end ETL (Extract, Transform, Load) data pipeline** using Python, AWS, and Terraform.

The pipeline:
- Extracts data from a source PostgreSQL database
- Stores raw data in Amazon S3
- Transforms the data into analytics-ready tables
- Loads the results into an AWS RDS PostgreSQL data warehouse

The solution is **serverless**, **event-driven**, and designed using **industry-standard data engineering practices**.

---

## ğŸ—ï¸ Architecture

### Pipeline Stages

**Week 1 â€“ Ingestion**
- Extract data from source PostgreSQL (Totesys)
- Incremental ingestion using timestamps
- Store raw JSON files in S3 (Landing Zone)
- Triggered on a schedule using EventBridge

**Week 2 â€“ Transformation**
- Read raw data from S3
- Transform data into dimension and fact tables (star schema)
- Write Parquet files to S3 (Processed Zone)
- Triggered automatically by S3 events

**Week 3 â€“ Loading**
- Load transformed Parquet files into RDS PostgreSQL
- Dimensions are upserted
- Fact tables are appended
- Warehouse ready for analytics and BI tools

---

## â˜ï¸ AWS Services Used

- AWS Lambda
- Amazon S3
- Amazon RDS (PostgreSQL)
- AWS Secrets Manager
- Amazon EventBridge
- Amazon CloudWatch
- Amazon VPC
- Terraform

---

## ğŸ§° Technologies & Versions

| Technology | Version |
|-----------|--------|
| Python | 3.11 |
| Terraform | >= 1.0 |
| PostgreSQL | 14 |
| pg8000 | 1.31.5 |
| pandas | AWS Lambda Layer |
| pyarrow | AWS Lambda Layer |

---

## ğŸ“ Project Structure

.
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ ingestion/
â”‚ â”œâ”€â”€ transformation/
â”‚ â”œâ”€â”€ loading/
|
â”œâ”€â”€ lambda_layer/
â”‚ â”œâ”€â”€ python/
â”‚ â”œâ”€â”€ lambda_layer.zip
â”‚ 
|
â”œâ”€â”€ test/
â”‚ â”œâ”€â”€ ingestion/
â”‚ â”œâ”€â”€ transformation/
| |â”€â”€ loading/
â”‚
â”œâ”€â”€ terraform/
â”‚ â”œâ”€â”€ main.tf
â”‚ â”œâ”€â”€ variables.tf
â”‚ â”œâ”€â”€ vpc.tf
â”‚ â”œâ”€â”€ s3.tf
â”‚ â”œâ”€â”€ rds.tf
â”‚ â”œâ”€â”€ iam.tf
â”‚ â”œâ”€â”€ lambda_ingestion.tf
â”‚ â”œâ”€â”€ lambda_transform.tf
â”‚ â”œâ”€â”€ lambda_loading.tf
â”‚ â”œâ”€â”€ cloudwatch.tf
â”‚ â”œâ”€â”€ eventbridge.tf
â”‚ â”œâ”€â”€ lambda_layer.tf
â”‚ â”œâ”€â”€ outputs.tf
â”‚ â”œâ”€â”€ terraform.tfvars
â”‚ â”œâ”€â”€ secrets.tf
â”‚ â”œâ”€â”€ s3_triggers.tf
â”‚ 
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ pytest.ini
â”œâ”€â”€ pyproject.toml
â””â”€â”€ README.md



---

## âš™ï¸ Setup Instructions

### 1ï¸âƒ£ Clone the repository

```bash
git clone <REPO_URL>
cd Data-Engineering-Group-Proj


### 2ï¸âƒ£ Create and activate a virtual environment

python3.11 -m venv .venv
source .venv/bin/activate

### 3ï¸âƒ£ Install Python dependencies (for local development & testing)

pip install --upgrade pip
pip install -r requirements.txt

### 4ï¸âƒ£ Export required Terraform secrets

export TF_VAR_totesys_db_password="your_source_db_password"
export TF_VAR_dw_db_password="your_warehouse_db_password"


### 5ï¸âƒ£ Initialise Terraform

cd terraform
terraform init

### 6ï¸âƒ£ Review infrastructure changes

terraform plan


### 7ï¸âƒ£ Deploy infrastructure

terraform apply

### ğŸ§¹ Teardown

terraform destroy




## â–¶ï¸ Running the Pipeline

Ingestion

- Runs automatically every 15 minutes

- Can be manually triggered via AWS Lambda console

Transformation

- Automatically triggered when new JSON files arrive in the landing S3 bucket

Loading

- Automatically triggered when new Parquet files arrive in the processed S3 bucket


## ğŸ§ª Running Tests

### From the project root:

pytest

### Run specific test folders:

pytest test/ingestion
pytest test/transformation



## ğŸ§¹ Code Quality & Security Checks

black src test
flake8 src
bandit -r src
pip-audit


## ğŸ§  Design Decisions

- Infrastructure defined using Terraform for repeatability

- Event-driven architecture using S3 triggers

- Secrets Manager used instead of hardcoded credentials

- Star schema for analytics-ready warehouse

- pg8000 used for Lambda-safe PostgreSQL connections

- AWS-managed Lambda Layers used to reduce deployment size








